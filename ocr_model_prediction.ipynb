{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "   \n",
    "    print(a)\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_data(data_path, target_path, encoder=None, fit_encoder=False):\n",
    "    X = pd.read_csv(data_path, header=None).values.astype(\"float32\")\n",
    "    y = pd.read_csv(target_path, header=None).values.squeeze()\n",
    "\n",
    "    # normalizza i dati\n",
    "    X = X / 255.0\n",
    "    \n",
    "    # reshape a immagine 16x8x1\n",
    "    X = X.reshape((-1, 16, 8, 1))\n",
    "\n",
    "    # encode labels\n",
    "    if fit_encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        y = encoder.fit_transform(y)\n",
    "    else:\n",
    "        y = encoder.transform(y)\n",
    "    \n",
    "    y = to_categorical(y)  # one-hot\n",
    "    \n",
    "    return X, y, encoder\n",
    "\n",
    "data_path = Path(\"ocr\")\n",
    "# train\n",
    "train_x, train_y, encoder = load_data(\n",
    "    data_path.joinpath(\"train-data.csv\").as_posix(), \n",
    "    data_path.joinpath(\"train-target.csv\").as_posix(),\n",
    "    fit_encoder=True\n",
    ")\n",
    "\n",
    "# test\n",
    "test_x, test_y, _ = load_data(\n",
    "    data_path.joinpath(\"test-data.csv\").as_posix(), \n",
    "    data_path.joinpath(\"test-target.csv\").as_posix(),\n",
    "    encoder=encoder\n",
    ")\n",
    "\n",
    "num_classes = train_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.gray()\n",
    "plt.matshow(255 - train_data[0]) # 255 - x simply inverts the fading direction of the image\n",
    "plt.show()\n",
    "# train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "alfa=list(train_class[0].value_counts().index)\n",
    "alfa.sort()\n",
    "alfa_dict_num_to_char={i:char for i,char in enumerate(alfa)}\n",
    "alfa_dict_char_to_num={char:i for i,char in enumerate(alfa)}\n",
    "\n",
    "train_class_conv=np.zeros((train_class.shape[0],26),dtype=int)\n",
    "test_class_conv=np.zeros((test_class.shape[0],26),dtype=int)\n",
    "\n",
    "for i,char in enumerate(train_class[0]):\n",
    "    train_class_conv[i][alfa_dict_char_to_num[char]]=1\n",
    "    \n",
    "for i,char in enumerate(test_class[0]):\n",
    "    test_class_conv[i][alfa_dict_char_to_num[char]]=1\n",
    "    \n",
    "train_y=tf.convert_to_tensor(train_class_conv)\n",
    "test_y=tf.convert_to_tensor(test_class_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_train_x,ms_val_x=tf.split(train_x,[int(len(train_x)*0.7),len(train_x)-int(len(train_x)*0.7)])\n",
    "ms_train_y,ms_val_y=tf.split(train_y,[int(len(train_y)*0.7),len(train_y)-int(len(train_y)*0.7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Softmax, ReLU\n",
    "\n",
    "class HWCharDeepModelKeras(Model):\n",
    "    def __init__(self, k):\n",
    "        super().__init__()\n",
    "        # Input shape: (batch, 16, 8, 1)\n",
    "        self.conv1 = Conv2D(\n",
    "            filters=32, kernel_size=k, padding='same',\n",
    "            activation='relu', kernel_initializer='he_normal'\n",
    "        )  # output: (16, 8, 32)\n",
    "        self.pool1 = MaxPooling2D(pool_size=(2, 2))    # output: (8, 4, 32)\n",
    "\n",
    "        self.conv2 = Conv2D(\n",
    "            filters=64, kernel_size=k, padding='same',\n",
    "            activation='relu', kernel_initializer='he_normal'\n",
    "        )  # output: (8, 4, 64)\n",
    "        self.pool2 = MaxPooling2D(pool_size=(2, 2))    # output: (4, 2, 64)\n",
    "\n",
    "        self.conv3 = Conv2D(\n",
    "            filters=128, kernel_size=k, padding='same',\n",
    "            activation='relu', kernel_initializer='he_normal'\n",
    "        )  # output: (4, 2, 128)\n",
    "        self.pool3 = MaxPooling2D(pool_size=(2, 2))    # output: (2, 1, 128)\n",
    "\n",
    "        self.flatten = Flatten()                        # output: (2*1*128 = 256)\n",
    "        self.fc1 = Dense(512, activation='relu', kernel_initializer='he_normal')\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.fc2 = Dense(26, kernel_initializer='he_normal')\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_sel(epochs,  \n",
    "              max_node_number, \n",
    "              batch_size_max, \n",
    "              lrs, \n",
    "              ms_train_x, ms_train_y, \n",
    "              ms_val_x, ms_val_y, \n",
    "              select_n_node=False, \n",
    "              select_bs=False):\n",
    "    \"\"\"\n",
    "    Esegue model selection testando diverse combinazioni di nodi, batch size e learning rate.\n",
    "\n",
    "    epochs: numero di epoche\n",
    "    max_node_number: massimo numero di filtri nei layer convoluzionali\n",
    "    batch_size_max: massimo esponente di 2 per il batch size (2,4,8,...)\n",
    "    lrs: minimo esponente negativo di 10 per il learning rate (10^-2,...)\n",
    "    \"\"\"\n",
    "\n",
    "    max_acc = 0\n",
    "    min_loss = np.inf\n",
    "    best_params = None\n",
    "\n",
    "    # Liste di iperparametri\n",
    "    node_list = range(5, max_node_number+1, 5) if select_n_node else [max_node_number]\n",
    "    batch_list = range(6, batch_size_max+1) if select_bs else [batch_size_max]\n",
    "    lr_list = [10**(-i) for i in range(2, lrs+1)]\n",
    "\n",
    "    for node_number in node_list:\n",
    "        print(f\"\\nNode number = {node_number}\")\n",
    "\n",
    "        for batch_exp in batch_list:\n",
    "            batch_train = 2**(batch_exp+1)\n",
    "            batch_val = 2**batch_exp\n",
    "            print(f\"   Batch size → train: {batch_train}, val: {batch_val}\")\n",
    "\n",
    "            # Dataset\n",
    "            train_ds = tf.data.Dataset.from_tensor_slices((ms_train_x, ms_train_y)) \\\n",
    "                                      .shuffle(buffer_size=len(ms_train_x)) \\\n",
    "                                      .batch(batch_train)\n",
    "            val_ds = tf.data.Dataset.from_tensor_slices((ms_val_x, ms_val_y)) \\\n",
    "                                    .batch(batch_val)\n",
    "\n",
    "            for lr in lr_list:\n",
    "                print(f\"      Learning rate = {lr}\")\n",
    "\n",
    "                # Inizializza nuovo modello\n",
    "                model = HWCharDeepModelKeras(node_number)\n",
    "                model.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                    metrics=[\"accuracy\"]\n",
    "                )\n",
    "\n",
    "                # Allenamento\n",
    "                history = model.fit(\n",
    "                    train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                val_loss, val_acc = model.evaluate(val_ds, verbose=0)\n",
    "\n",
    "                # Salva best model\n",
    "                if val_acc*100 > max_acc and val_loss < min_loss:\n",
    "                    max_acc = val_acc*100\n",
    "                    min_loss = val_loss\n",
    "                    best_params = {\n",
    "                        \"node_number\": node_number,\n",
    "                        \"batch_train\": batch_train,\n",
    "                        \"batch_val\": batch_val,\n",
    "                        \"learning_rate\": lr,\n",
    "                        \"epochs\": epochs,\n",
    "                        \"val_acc\": max_acc,\n",
    "                        \"val_loss\": min_loss\n",
    "                    }\n",
    "\n",
    "                    print(f\"New best → {best_params}\")\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_hyp_params=model_sel(35,  5, 6, 5, ms_train_x, ms_train_y, ms_val_x, ms_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_sp_x,val_sp_x=tf.split(train_x,[int(len(train_x)*0.7),len(train_x)-int(len(train_x)*0.7)])\n",
    "train_sp_y,val_sp_y=tf.split(train_y,[int(len(train_y)*0.7),len(train_y)-int(len(train_y)*0.7)])\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_sp_x, train_sp_y)).shuffle(10000).batch(selected_hyp_params['batch_train'])\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_sp_x, val_sp_y)).shuffle(10000).batch(selected_hyp_params['batch_train'])\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(selected_hyp_params['batch_train']//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_hyp_params)\n",
    "EPOCHS = selected_hyp_params['epochs']\n",
    "network = HWCharDeepModelKeras(selected_hyp_params['node_number'])\n",
    "\n",
    "network.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=selected_hyp_params['learning_rate']),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = network.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "test_loss, test_acc = network.evaluate(test_ds)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class=network(test_x,training=False)\n",
    "\n",
    "list_of_predicted_class=[]\n",
    "for i in predicted_class:\n",
    "    ind=int(tf.argmax(i))\n",
    "    list_of_predicted_class.append(alfa_dict_num_to_char[ind])\n",
    "# list_of_predicted_class\n",
    "\n",
    "file = open('predicted_class.txt', 'w')\n",
    "\n",
    "for char in list_of_predicted_class:\n",
    "    file.write(char+'\\n')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, sharex=True)\n",
    "\n",
    "ax[0].plot(history.history[\"loss\"], label=\"train\")\n",
    "ax[0].plot(history.history[\"val_loss\"], label=\"test\")\n",
    "ax[0].set_ylabel(\"Cross-Entropy loss\")\n",
    "ax[0].set_ylim(0)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(history.history[\"accuracy\"], label=\"train\")\n",
    "ax[1].plot(history.history[\"val_accuracy\"], label=\"test\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_ylim(0, 1)\n",
    "ax[1].legend()\n",
    "\n",
    "fig.suptitle(\"CNN Training History\")\n",
    "fig.savefig(\"cnn-training.png\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
